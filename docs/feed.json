[{"title":"Installation","permalink":"https://typhoondata.io/installation.html","link":"https://typhoondata.io/installation.html","date":"2022-07-16T00:00:00-07:00","modified":"2022-07-17T04:34:46-07:00","author":{"name":"Typhoon Data","url":"https://typhoondata.io","email":"info.typhoon.data@gmail.com"},"content":"","excerpt":"","languages":null,"categories":[],"tags":["getting-started"]},{"title":"Introducing Typhoon beta 🎊","permalink":"https://typhoondata.io/typhoon-beta-release.html","link":"https://typhoondata.io/typhoon-beta-release.html","date":"2022-07-16T00:00:00-07:00","modified":"2022-07-25T22:22:06-07:00","author":{"name":"Typhoon Data","url":"https://typhoondata.io","email":"info.typhoon.data@gmail.com"},"content":"<h1 id=\"introducing-typhoon\">Introducing Typhoon</h1>\n\n<p>We are pleased to introduce Typhoon Orchestrator! It’s Open Source and ready to use.  We hope you like the elegant YAML, and how simple and productive it is to use!</p>\n\n<p>Our key takeaways are:</p>\n<ul>\n  <li>Easily extend with pure python. Framework-less, with no dependencies.</li>\n  <li>Deploy to your existing Airflow. <strong>No risk migration!</strong></li>\n  <li>Deploy to AWS Lambda. <strong>Serveless is the future!</strong></li>\n</ul>\n\n<p><a href=\"https://typhoon-data-org.github.io/website/serverless-telegram-bot-jokes.html\">See our blog on making a Telegram bot with AWS Lambda and Typhoon.</a></p>\n\n<h1 id=\"our-vision\">Our vision</h1>\n\n<p>We are fully Open Source, including plugins, and are committed to good Open Source practices.</p>\n\n<p>Our vision is to make Data Engineering 10x more productive. Our approach has been to try not to break conventions without good reason. Many things should be immediately familiar to anyone that has worked with <strong>Airflow</strong>. Just like Snowflake did not reinvent SQL, we want to make you more productive without the pain of a steep learning curve.</p>\n\n<p>We wanted to make a next generation, cloud native, and asynchronous Orchestrator that can handle highly dynamic workflows with ease. We crafted Typhoon from the ground up to work towards this vision. It’s designed to feel familiar while still making very different design decisions where it matters.</p>\n\n<p>More on <a href=\"https://typhoon-data-org.github.io/website/typhoon-orchestrator-vision.html\">why Typhoon</a>.</p>\n\n<h3 id=\"key-features\">Key features</h3>\n\n<ul>\n  <li><strong>Pure python</strong> - Easily extend with pure python. Frameworkless, with no dependencies.</li>\n  <li><strong>Testable Python</strong> - Write tests for your tasks in PyTest. Automate DAG testing.</li>\n  <li><strong>Composability</strong> - Functions and connections combine like Lego. Very easy to extend.</li>\n  <li><strong>Data sharing</strong> - data flows between tasks making it intuitive to build tasks.</li>\n  <li><strong>Elegant: YAML</strong> - low-code and easy to learn.</li>\n  <li><strong>Code-completion</strong> - Fast to compose. (VS Code recommended).</li>\n  <li><strong>Components</strong> - reduce complex tasks (e.g. CSV → S3 → Snowflake) to 1 re-usable task.</li>\n  <li><strong>Components UI</strong> -  Share your pre-built automation with your team. teams. :raised_hands:</li>\n  <li><strong>Rich Cli &amp; Shell</strong> - Inspired by other great command line interfaces and instantly familiar. Intelligent bash/zsh completion.</li>\n  <li><strong>Flexible deployment</strong> - Deploy to Airflow. Large reduction in effort, without breaking existing production.</li>\n</ul>\n\n<h1 id=\"example-yaml-dag\">Example YAML DAG</h1>\n\n<figure class=\"highlight\"><pre><code class=\"language-yaml\" data-lang=\"yaml\"><table class=\"rouge-table\"><tbody><tr><td class=\"gutter gl\"><pre class=\"lineno\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n</pre></td><td class=\"code\"><pre><span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">favorite_authors</span>\n<span class=\"na\">schedule_interval</span><span class=\"pi\">:</span> <span class=\"s\">rate(1 day)</span>\n\n<span class=\"na\">tasks</span><span class=\"pi\">:</span>\n  <span class=\"na\">choose_favorites</span><span class=\"pi\">:</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.flow_control.branch</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">branches</span><span class=\"pi\">:</span>\n        <span class=\"pi\">-</span> <span class=\"s\">J. K. Rowling</span>\n        <span class=\"pi\">-</span> <span class=\"s\">George R. R. Martin</span>\n        <span class=\"pi\">-</span> <span class=\"s\">James Clavell</span>\n\n  <span class=\"na\">get_author</span><span class=\"pi\">:</span>\n    <span class=\"na\">input</span><span class=\"pi\">:</span> <span class=\"s\">choose_favorites</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">functions.open_library_api.get_author</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">author</span><span class=\"pi\">:</span> <span class=\"kt\">!Py</span> <span class=\"s\">$BATCH</span>\n</pre></td></tr></tbody></table></code></pre></figure>\n\n<h1 id=\"production-use-cases\">Production use cases</h1>\n\n<p>Typhoon with Airflow has been in production at a medium sized hotel chain since 2021. It has been running on Airflow <strong>in production for over a year with no maintenance or hiccups</strong>. We have only modified the code each time we added a new data source.</p>\n\n<h1 id=\"roadmap\">Roadmap</h1>\n\n<p>We are working on gathering feedback on our current release. Please do reach out to us and start a conversation at @DataTyphoon.</p>\n\n<p>Next, we are working on making it even easier to deploy to AWS Lambda and to make better documentation and auto-completion for the plugins. This will make using it even easier without needing to refer to the functions.</p>\n\n<p>You can find out more about the project at: \n<a href=\"https://github.com/typhoon-data-org/typhoon-orchestrator\">Github Readme</a>\n<a href=\"https://typhoon-data-org.github.io/typhoon-orchestrator/getting-started/installation/\">Docs</a></p>","excerpt":"Introducing Typhoon","languages":null,"categories":["article"],"tags":["typhoon","release","beta","article","history"]},{"title":"Hello World - 5 min walkthrough","permalink":"https://typhoondata.io/hello-world.html","link":"https://typhoondata.io/hello-world.html","date":"2022-07-15T00:00:00-07:00","modified":"2022-07-17T04:34:46-07:00","author":{"name":"Typhoon Data","url":"https://typhoondata.io","email":"info.typhoon.data@gmail.com"},"content":"","excerpt":"","languages":null,"categories":[],"tags":["getting-started"]},{"title":"Why Typhoon?","permalink":"https://typhoondata.io/typhoon-orchestrator-vision.html","link":"https://typhoondata.io/typhoon-orchestrator-vision.html","date":"2022-07-15T00:00:00-07:00","modified":"2022-07-25T22:22:06-07:00","author":{"name":"Typhoon Data","url":"https://typhoondata.io","email":"info.typhoon.data@gmail.com"},"content":"<h1 id=\"history-of-typhoon\">History of Typhoon</h1>\n\n<p>Long before I ever wrote the first line of code, the kernel of the idea that would eventually grow into Typhoon had been planted in my mind. It was 2017 and I was working as a data engineer in a large travel tech company. We were working with almost every kind of data engineering project you can imagine:</p>\n<ul>\n  <li>batch processes using Hadoop on Amazon EMR, AWS Lambdas</li>\n  <li>moving gigabytes of data per hour on real time workflows with Spark, Kinesis and Kafka</li>\n  <li>visual tools like Talend and Data Services</li>\n  <li>landing data in an S3 data lake</li>\n  <li>ingesting into cloud data warehouse like Redshift and eventually Snowflake.</li>\n</ul>\n\n<p>Throughout all this change, one thing remained constant though: <strong>Airflow</strong>.</p>\n\n<p>Airflow remained in heavy use since the beginning and it proved to be the most versatile tool in our toolbox. We ended up using it to orchestrate all of our flows except for the few real time ones. The ability to easily write ETL/ELT in Python which was increasingly becoming the language of data, coupled with good visualization and monitoring capabilities was hard to beat. And yet for all its strengths, I still found myself frequently wondering if there could be a better way to do things.</p>\n\n<h1 id=\"is-there-a-better-way-than-airflow\">Is there a better way than Airflow?</h1>\n\n<p>After giving it much thought and having many lunch time conversations with my coworkers I became convinced that all my issues with Airflow stemmed from the same root cause: task isolation. What in theory looked like a good thing endes up hindering good software practices. When tasks don’t share data, separation of concerns is not possible. This means that what should be two separate tasks, like for example reading from a source and writing to a destination, end up coupled into a single one. This greatly minimizes the reusability of your operators. Worse than that, it forces your DAG structure to be static, and we found more than a few use cases that needed dynamic workflows forcing us to work around Airflow’s limitations. One thing I did know from the beginning is that whatever framework eventually replaces Airflow (if any) needs to be open source and easy to extend in python.</p>\n\n<p>Drawing inspiration from Golang’s channels, I envisioned data flows where each task would be truly independent and send batches of data through to downstream tasks as soon as it was available. This would mean that a DAG would no longer be a strictly sequential entity where task A needs to execute fully before task B can start. Instead all tasks could be running at once, and batches of data would flow freely between them at all times. This is not unlike a conveyor belt in a factory delivering each finished piece and keeping everyone busy [1].</p>\n\n<h1 id=\"vision-for-serverless-orchestration\">Vision for Serverless Orchestration</h1>\n\n<p>By the start of 2019 the idea was clear enough in my mind that I wrote a proof of concept using python generators and a queue between processes. The end result was more complicated than it needed to be and not very user friendly, but I could see the potential in my mind and started to iterate.</p>\n\n<p>One of the big turning points came after reading the Zappa [2] readme and realizing that the framework could be used to schedule tasks in lambda.  Even more interesting, by decorating a function with <code class=\"language-plaintext highlighter-rouge\">@async</code> you could make it call a new instance of itself and run in parallel. All of this coupled with the fact that AWS has its own scheduler to trigger a function on a schedule made me wonder… could Zappa be the perfect backend for an orchestrator? I set out to modify my code to create a new POC removing all queues and using Zappa, and while the framework ended up having some limitations that forced me pull it out and rewrite my own backend, the idea of deploying to lambda was a breakthrough that was here to stay.</p>\n\n<blockquote>\nPerfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.\n</blockquote>\n\n<p>Fast forward to now and although the core ideas have not changed at all, Typhoon in its current form is much more user friendly and less complex than it used to be. Every concept that did not add value was removed until we were left with a much simpler framework, without sacrificing power. In the concepts that stayed we tried not to break conventions without good reason, so many things should be immediately familiar to anyone that has worked with Airflow. We believe the best products are the ones that innovate in execution without forcing you to learn a completely new paradigm. A good example is Snowflake which is radically different to a classical data warehouse, but still has a shallow learning curve. They would likely not have been as successful if they had reinvented SQL.</p>\n\n<p>Once the basic framework was built and proved to be versatile enough, the rest was just adding icing on the cake:</p>\n\n<ul>\n  <li>Rich CLI</li>\n  <li>Extremely easy to extend functions in plain python</li>\n  <li>Grouping tasks into components</li>\n  <li>Generating terraform scripts for deployment</li>\n</ul>\n\n<h1 id=\"where-first-airflow\">Where first? Airflow!</h1>\n\n<p>But even though we firmly believe our product is an big evolutary step over Airflow, we also recognise how ubiquitous Airflow is. It’s the industry standard and has a huge user base. More than any other tool, every data engineering department seems to have an Airflow instance running somewhere. Even if we could succeed in providing a better experience, was it realistic to expect that many people to rewrite their pipelines in a new tool? Our conclusion was that it’s just <em>not realistic</em> for most as it would be too costly. So what could we do? Well <strong>if you can’t beat them, join them</strong>!</p>\n\n<p>This is when having the DAG layer in YAML really paid dividends. As we were already compiling into Lambda compatible python, the workflow logic was already separate from the execution backend. It couldn’t be too hard to compile into Airflow DAGs. You would sacrifice the parallelism that Lambda can provide, but in exchange you could slowly migrate into Typhoon while keeping old flows still working in pure Airflow. The end result was a level of integration far beyond what we’ve seen in other tools, which commonly just create a DAG with one task that then calls their framework. Typhoon actually translates each task into an Airflow one, can join two tasks together if they’ll be sharing data that you don’t want to send over the network, and can intelligently split branches. Moreover, Typhoon’s variables and connections directly integrate with Airflow’s ones so they’re easy to manage. Typhoon on Airflow is a first class citizen that doesn’t feel alien.</p>\n\n<p>In fact this was our chosen method when we deployed Typhoon in production for the first time at a medium sized hotel chain. It has been running on Airflow <strong>in production for over a year with no maintenance or hiccups</strong>, and we have only had to modify the code each time a new data source was requested.</p>\n\n<h1 id=\"so-what-do-you-think\">So, what do you think?</h1>\n\n<p>Although we have hundreds of ideas of features we could implement (web UI, new triggers apart from schedules like S3 events or Kafka messages, monitoring and smart alerting etc.) we feel that we are at a stage where the product is mature enough that we would benefit from feedback while moving forward, while still being early stage enough to influence the direction. We are very excited for you to give it a try and hear your thoughts!</p>\n\n<p>Start a conversation with us -  @DataTyphoon.</p>\n\n<h3 id=\"footnotes\">Footnotes</h3>\n\n<ul>\n  <li>[1] <a href=\"https://beam.apache.org/\">Apache Beam</a> is built on similar ideas but went in a very different direction</li>\n  <li>[2] <a href=\"https://github.com/Miserlou/Zappa\">Miserlou Zappa Github</a></li>\n</ul>","excerpt":"History of Typhoon","languages":null,"categories":["article"],"tags":["typhoon","serverless","article","history"]},{"title":"Airflow; Standing on the shoulders of giants","permalink":"https://typhoondata.io/Airflow.html","link":"https://typhoondata.io/Airflow.html","date":"2022-07-02T00:00:00-07:00","modified":"2022-07-17T04:34:46-07:00","author":{"name":"Typhoon Data","url":"https://typhoondata.io","email":"info.typhoon.data@gmail.com"},"content":"<h4 id=\"table-of-contents\">Table of contents</h4>\n<ul id=\"markdown-toc\">\n  <li><a href=\"#table-of-contents\" id=\"markdown-toc-table-of-contents\">Table of contents</a></li>\n  <li><a href=\"#built-for-developers\" id=\"markdown-toc-built-for-developers\">Built for developers</a></li>\n  <li><a href=\"#testable\" id=\"markdown-toc-testable\">Testable</a></li>\n  <li><a href=\"#composable\" id=\"markdown-toc-composable\">Composable</a></li>\n  <li><a href=\"#extensible\" id=\"markdown-toc-extensible\">Extensible</a></li>\n  <li><a href=\"#quick-feedback\" id=\"markdown-toc-quick-feedback\">Quick feedback</a></li>\n  <li><a href=\"#debugging\" id=\"markdown-toc-debugging\">Debugging</a></li>\n  <li><a href=\"#try-it-out\" id=\"markdown-toc-try-it-out\">Try it out!</a></li>\n</ul>\n\n<p>Airflow advanced the state of the art in ETL tools by providing an extremely flexible and reliable framework. It is easy to monitor your jobs and you can extend it with plugins to do anything that python can do. It also helped introduce the concept of functional batch data pipelines. By removing state from pipelines and enforcing strict boundaries on partitions of time you can more easily reprocess a partition of data without affecting the rest of it. If you’re unfamiliar with that concept, <a href=\"https://maximebeauchemin.medium.com/functional-data-engineering-a-modern-paradigm-for-batch-data-processing-2327ec32c42a\">this article</a> by Airflow’s creator Maxime Beauchemin is worth a read.</p>\n\n<p>With <a href=\"https://github.com/typhoon-data-org/typhoon-orchestrator\">Typhoon</a> we aim to build on this concept to provide a framework with a stronger focus on software engineering principles. We will illustrate it in the following sections.</p>\n\n<h2 id=\"built-for-developers\">Built for developers</h2>\n\n<p>Typhoon was built from the ground up to provide a great experience for developers. Besides providing great <a href=\"https://typhoon-data-org.github.io/typhoon-orchestrator/index.html#auto-completion\">Intellisense</a> it helps you implement software best practices.</p>\n\n<h2 id=\"testable\">Testable</h2>\n\n<p>Airflow is notoriously hard to test. Operators force coupling between logic, execution context and framework. Let’s look at a really simple example:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">class</span> <span class=\"nc\">ExchangeRates</span><span class=\"p\">(</span><span class=\"n\">BaseOperator</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">base</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">symbols</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"bp\">None</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">base</span> <span class=\"o\">=</span> <span class=\"n\">base</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">symbols</span> <span class=\"o\">=</span> <span class=\"n\">symbols</span>\n        <span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">http_conn_id</span> <span class=\"o\">=</span> <span class=\"n\">http_conn_id</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">execute</span><span class=\"p\">(</span><span class=\"n\">context</span><span class=\"p\">):</span>\n        <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n            <span class=\"s\">'start_at'</span><span class=\"p\">:</span> <span class=\"n\">context</span><span class=\"p\">[</span><span class=\"s\">'execution_date'</span><span class=\"p\">],</span>\n            <span class=\"s\">'end_at'</span><span class=\"p\">:</span> <span class=\"n\">context</span><span class=\"p\">[</span><span class=\"s\">'next_execution_date'</span><span class=\"p\">],</span>\n        <span class=\"p\">}</span>\n        <span class=\"n\">full_endpoint</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s\">'</span><span class=\"si\">{</span><span class=\"n\">ENDPOINT</span><span class=\"si\">}</span><span class=\"s\">/history'</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">'Calling endpoint </span><span class=\"si\">{</span><span class=\"n\">full_endpoint</span><span class=\"si\">}</span><span class=\"s\"> for dates between </span><span class=\"si\">{</span><span class=\"n\">start_at</span><span class=\"si\">}</span><span class=\"s\">, </span><span class=\"si\">{</span><span class=\"n\">end_at</span><span class=\"si\">}</span><span class=\"s\">'</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">base</span><span class=\"p\">:</span>\n            <span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s\">'base'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">base</span>\n        <span class=\"k\">if</span> <span class=\"n\">symbols</span><span class=\"p\">:</span>\n            <span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s\">'symbols'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">symbols</span>\n        <span class=\"n\">hook</span> <span class=\"o\">=</span> <span class=\"n\">HttpsHook</span><span class=\"p\">(</span><span class=\"s\">'get'</span><span class=\"p\">,</span> <span class=\"n\">http_conn_id</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"p\">.</span><span class=\"n\">http_conn_id</span><span class=\"p\">)</span>\n        <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">hook</span><span class=\"p\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">full_endpoint</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"n\">params</span><span class=\"p\">)</span>\n        <span class=\"n\">context</span><span class=\"p\">[</span><span class=\"s\">'task_instance'</span><span class=\"p\">].</span><span class=\"n\">xcom_push</span><span class=\"p\">(</span><span class=\"s\">'response'</span><span class=\"p\">,</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">json</span><span class=\"p\">())</span>\n</code></pre></div></div>\n\n<p>All your logic is in the execute function of your opertor, so in order to run a test you need to import airflow and create an instance of the operator. Not only that, but we need to provide a context similar to the one that airflow would provide. Finally, you would need to mock xcom and see that it’s called with the value you expect it is. This is only a simple example but it can get much more complex once there is a source and a destination in the same component, magic macro rendering and more. Just in case this doesn’t sound complex enough, notice we create a hook from its connection id. Yeah, you’ll need to mock the airflow database too or spin up a temporary one. Good luck with that.</p>\n\n<p>In contrast, the logic for typhoon tasks lives inside regular python functions. They don’t make use of the framework unless they use a hook and even then it can be instantiated without a metadata database.</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code> <span class=\"k\">def</span> <span class=\"nf\">get_exchange_rates</span><span class=\"p\">(</span>\n        <span class=\"n\">hook</span><span class=\"p\">:</span> <span class=\"n\">HTTPHook</span>\n        <span class=\"n\">start_at</span><span class=\"p\">:</span><span class=\"n\">datetime</span><span class=\"p\">,</span>\n        <span class=\"n\">end_at</span><span class=\"p\">:</span> <span class=\"n\">datetime</span><span class=\"p\">,</span>\n        <span class=\"n\">base</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"bp\">None</span><span class=\"p\">,</span>\n        <span class=\"n\">symbols</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"bp\">None</span><span class=\"p\">,</span>\n<span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">dict</span><span class=\"p\">:</span>\n    <span class=\"n\">params</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"s\">'start_at'</span><span class=\"p\">:</span> <span class=\"n\">start_at</span><span class=\"p\">,</span>\n        <span class=\"s\">'end_at'</span><span class=\"p\">:</span> <span class=\"n\">end_at</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n    <span class=\"n\">full_endpoint</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"s\">'</span><span class=\"si\">{</span><span class=\"n\">ENDPOINT</span><span class=\"si\">}</span><span class=\"s\">/history'</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">'Calling endpoint </span><span class=\"si\">{</span><span class=\"n\">full_endpoint</span><span class=\"si\">}</span><span class=\"s\"> for dates between </span><span class=\"si\">{</span><span class=\"n\">start_at</span><span class=\"si\">}</span><span class=\"s\">, </span><span class=\"si\">{</span><span class=\"n\">end_at</span><span class=\"si\">}</span><span class=\"s\">'</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">base</span><span class=\"p\">:</span>\n        <span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s\">'base'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">base</span>\n    <span class=\"k\">if</span> <span class=\"n\">symbols</span><span class=\"p\">:</span>\n        <span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s\">'symbols'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">symbols</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"p\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">full_endpoint</span><span class=\"p\">,</span> <span class=\"n\">params</span><span class=\"o\">=</span><span class=\"n\">params</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">json</span><span class=\"p\">()</span>\n</code></pre></div></div>\n\n<p>Testing this is as easy as:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">def</span> <span class=\"nf\">test_xr_get_history</span><span class=\"p\">():</span>\n    <span class=\"n\">symbols</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s\">'EUR'</span><span class=\"p\">,</span> <span class=\"s\">'PHP'</span><span class=\"p\">,</span> <span class=\"s\">'HKD'</span><span class=\"p\">]</span>\n    <span class=\"n\">start_at</span> <span class=\"o\">=</span> <span class=\"n\">date</span><span class=\"p\">(</span><span class=\"mi\">2020</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">end_at</span> <span class=\"o\">=</span> <span class=\"n\">date</span><span class=\"p\">(</span><span class=\"mi\">2020</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span>\n    <span class=\"n\">hook</span> <span class=\"o\">=</span> <span class=\"n\">HTTPSHook</span><span class=\"p\">(</span><span class=\"n\">ConnParams</span><span class=\"p\">(</span><span class=\"n\">conn_type</span><span class=\"o\">=</span><span class=\"s\">'https_hook'</span><span class=\"p\">,</span> <span class=\"n\">extra</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s\">'method'</span><span class=\"p\">:</span> <span class=\"s\">'get'</span><span class=\"p\">}))</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">exchange_rates_api</span><span class=\"p\">.</span><span class=\"n\">get_history</span><span class=\"p\">(</span>\n        <span class=\"n\">hook</span><span class=\"o\">=</span><span class=\"n\">hook</span><span class=\"p\">,</span>\n        <span class=\"n\">start_at</span><span class=\"o\">=</span><span class=\"n\">start_at</span><span class=\"p\">,</span>\n        <span class=\"n\">end_at</span><span class=\"o\">=</span><span class=\"n\">end_at</span><span class=\"p\">,</span>\n        <span class=\"n\">base</span><span class=\"o\">=</span><span class=\"s\">'USD'</span><span class=\"p\">,</span>\n        <span class=\"n\">symbols</span><span class=\"o\">=</span><span class=\"n\">symbols</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">keys</span><span class=\"p\">())</span> <span class=\"o\">==</span> <span class=\"p\">{</span><span class=\"s\">'rates'</span><span class=\"p\">,</span> <span class=\"s\">'start_at'</span><span class=\"p\">,</span> <span class=\"s\">'end_at'</span><span class=\"p\">,</span> <span class=\"s\">'base'</span><span class=\"p\">}</span>\n    <span class=\"k\">assert</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s\">'rates'</span><span class=\"p\">].</span><span class=\"n\">keys</span><span class=\"p\">())</span> <span class=\"o\">==</span> <span class=\"p\">{</span><span class=\"n\">start_at</span><span class=\"p\">.</span><span class=\"n\">isoformat</span><span class=\"p\">(),</span> <span class=\"n\">end_at</span><span class=\"p\">.</span><span class=\"n\">isoformat</span><span class=\"p\">()}</span>\n    <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"n\">response</span><span class=\"p\">[</span><span class=\"s\">'rates'</span><span class=\"p\">].</span><span class=\"n\">items</span><span class=\"p\">():</span>\n        <span class=\"k\">assert</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">.</span><span class=\"n\">keys</span><span class=\"p\">())</span> <span class=\"o\">==</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">symbols</span><span class=\"p\">)</span>\n</code></pre></div></div>\n\n<p>We don’t need to import the framework, mock anything or have a database running. We just give it some input and test the output. This takes the functional aspect in functional data pipelines even further.</p>\n\n<h2 id=\"composable\">Composable</h2>\n\n<p>Composability is one of the principles of good software engineering because it enables you to reuse existing functions or objects in order to achieve new behaviour. Airflow gets in the way of that by coupling context, as we explained in the previous section, but also by encouraging task isolation. Tasks can’t pass data between them, only some metadata through XCom and even that is discouraged. That means that you can’t have an FTPExtractOperator and an S3LoadOperator, you need an FTPToS3Operator and every other possible combination of sources and destinations. This does not compose well as you end up with a lot of repeated code across different operators just because you can’t easily reuse the logic.</p>\n\n<p>In typhoon tasks can pass any data between them without any performance penalty. You can have a function that extracts data from a source and another one that loads into a destination. You can reuse those functions in any other DAG that uses that source or destination.</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">example</span>\n<span class=\"na\">schedule_interval</span><span class=\"pi\">:</span> <span class=\"s\">rate(1 day)</span>\n\n<span class=\"na\">tasks</span><span class=\"pi\">:</span>\n<span class=\"na\">extract_files</span><span class=\"pi\">:</span>\n<span class=\"na\">component</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.get_data_from_files</span>\n<span class=\"na\">args</span><span class=\"pi\">:</span>\n<span class=\"na\">hook</span><span class=\"pi\">:</span> <span class=\"kt\">!Hook</span> <span class=\"s\">my_ftp</span>\n<span class=\"na\">pattern</span><span class=\"pi\">:</span> <span class=\"s\">/base/path/*.csv</span>\n\n<span class=\"na\">load_files</span><span class=\"pi\">:</span>\n<span class=\"na\">input</span><span class=\"pi\">:</span> <span class=\"s\">extract_files</span>\n<span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.filesystem.write_data</span>\n<span class=\"na\">args</span><span class=\"pi\">:</span>\n<span class=\"na\">hook</span><span class=\"pi\">:</span> <span class=\"kt\">!Hook</span> <span class=\"s\">my_s3</span>\n<span class=\"na\">data</span><span class=\"pi\">:</span> <span class=\"kt\">!Py</span> <span class=\"s\">$BATCH.data</span>\n<span class=\"na\">path</span><span class=\"pi\">:</span> <span class=\"kt\">!MultiStep</span>\n<span class=\"pi\">-</span> <span class=\"kt\">!Py</span> <span class=\"s\">typhoon.files.name($BATCH.path)</span>\n<span class=\"pi\">-</span> <span class=\"kt\">!Py</span> <span class=\"s\">f'/some/path/{$1}'</span>\n</code></pre></div></div>\n\n<h2 id=\"extensible\">Extensible</h2>\n\n<p>There are several ways in which the framework facilitates extension.</p>\n\n<ul>\n  <li><strong>Just python</strong>. One of typhoon’s goals is to be easily extensible with regular python code. You can create python functions and call them in your DAGs.</li>\n  <li><strong>Interfaces</strong>. Hooks are grouped into interfaces in a lot of cases where it makes sense to make them interchangeable. This means you can easily switch a hook that writes to files in your OS for local development into an S3 hook for the integration tests and production. More importantly, since a lot of functions take a hook of a specific interface, if you create a new hook that conforms to that interface it will automatically be compatible with all those functions.</li>\n  <li><strong>Natively support additional connection types</strong>. When you create a new kind of hook and give it a conn_type, this will be used to discriminate the class when a hook instance is created from a  connection defined in the metadata.</li>\n</ul>\n\n<h2 id=\"quick-feedback\">Quick feedback</h2>\n\n<p>Typhoon aims to provide a lightning fast feedback loop on all steps of the DAG creation process. From debug hooks that print whatever is passed to them, to interchangeable hooks so you can easily develop, test and deploy against different targets, to being able to run the whole DAG from the command line instead of needing to schedule it or run independent tasks.</p>\n\n<h2 id=\"debugging\">Debugging</h2>\n\n<p>Typhoon is designed from the ground up to be easy to debug and it achieves this by compiling to regular python that can be executed locally and debugged from your favorite IDE.</p>\n\n<h2 id=\"try-it-out\">Try it out!</h2>\n<p>If you’re curious on what the future of data pipelines could look like check out <a href=\"/\">Typhoon</a></p>\n\n<p>–\nBiel</p>","excerpt":"Table of contents\n\n  Table of contents","languages":null,"categories":["article"],"tags":["typhoon","airflow","article"]},{"title":"Modern data warehouse patterns. ELT with Snowflake variants","permalink":"https://typhoondata.io/Modern-stack.html","link":"https://typhoondata.io/Modern-stack.html","date":"2022-07-01T00:00:00-07:00","modified":"2022-07-17T04:34:46-07:00","author":{"name":"Typhoon Data","url":"https://typhoondata.io","email":"info.typhoon.data@gmail.com"},"content":"<h4 id=\"table-of-contents\">Table of contents</h4>\n<ul id=\"markdown-toc\">\n  <li><a href=\"#table-of-contents\" id=\"markdown-toc-table-of-contents\">Table of contents</a></li>\n  <li><a href=\"#leveraging-semi-structured-data-for-resilience-against-schema-changes\" id=\"markdown-toc-leveraging-semi-structured-data-for-resilience-against-schema-changes\">Leveraging semi-structured data for resilience against schema changes</a></li>\n  <li><a href=\"#real-world-example--personal-information\" id=\"markdown-toc-real-world-example--personal-information\">Real world example- Personal information</a>    <ul>\n      <li><a href=\"#creating-a-view\" id=\"markdown-toc-creating-a-view\">Creating a view</a></li>\n      <li><a href=\"#removing-a-column-adding-a-column\" id=\"markdown-toc-removing-a-column-adding-a-column\">Removing a column, adding a column</a></li>\n      <li><a href=\"#doesnt-this-take-up-more-space-than-regular-tables-isnt-it-slower-to-query\" id=\"markdown-toc-doesnt-this-take-up-more-space-than-regular-tables-isnt-it-slower-to-query\">Doesn’t this take up more space than regular tables? Isn’t it slower to query?</a></li>\n      <li><a href=\"#improving-performance\" id=\"markdown-toc-improving-performance\">Improving performance</a></li>\n    </ul>\n  </li>\n  <li><a href=\"#what-is-the-best-way-to-load-the-data\" id=\"markdown-toc-what-is-the-best-way-to-load-the-data\">What is the best way to load the data?</a></li>\n  <li><a href=\"#bottom-line\" id=\"markdown-toc-bottom-line\">Bottom Line</a></li>\n  <li><a href=\"#what-are-the-best-tools-to-load-data-like-this\" id=\"markdown-toc-what-are-the-best-tools-to-load-data-like-this\">What are the best tools to load data like this?</a></li>\n  <li><a href=\"#sources\" id=\"markdown-toc-sources\">Sources</a></li>\n</ul>\n\n<h2 id=\"leveraging-semi-structured-data-for-resilience-against-schema-changes\">Leveraging semi-structured data for resilience against schema changes</h2>\n\n<p>As data warehouse technologies get cheaper and better, ELT is gaining momentum over ETL. In this article we will show you how to leverage Snowflake’s semi-structured data to build integrations that are highly resistant to changes in schema while staying performant. Schema changes are one of the most common things that can break a data pipeline (adding and removing fields, changes in types or length of the data etc.) so it is extremely useful to protect yourself against them.</p>\n\n<h2 id=\"real-world-example--personal-information\">Real world example- Personal information</h2>\n\n<p>Let’s assume we have a table with basic information about our clients. The goal is to load the information into snowflake unchanged.</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>name</th>\n      <th>surname</th>\n      <th>age</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Anne</td>\n      <td>Houston</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <td>John</td>\n      <td>Doe</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <td>William</td>\n      <td>Williams</td>\n      <td>27</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>We would usually create the following table in Snowflake:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">clients</span> <span class=\"p\">(</span><span class=\"n\">name</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">,</span> <span class=\"n\">surname</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">,</span> <span class=\"n\">age</span> <span class=\"n\">NUMBER</span><span class=\"p\">);</span>\n</code></pre></div></div>\n\n<p>Notice how we don’t specify the varchar’s length or the number’s precision and scale. This is preferable because snowflake will automatically use the minimum size needed to store the data efficiently, and if the source system changes the length of a varchar, or the precision of a number your flows won’t break. An exception is when a number has decimals we will need to specify a precision and scale.</p>\n\n<p>But if we do that, our integration will fail if a field is removed, and if a field is added we won’t notice. We are not resilient to schema changes. To solve that we will instead create a table with just one variant field where we will load all the data, no matter what fields it has.</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">clients_raw</span> <span class=\"p\">(</span><span class=\"n\">src</span> <span class=\"n\">VARIANT</span><span class=\"p\">);</span>\n</code></pre></div></div>\n\n<p>In order to load the data we can dump it as JSON into a stage.</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">REPLACE</span> <span class=\"n\">FILE</span> <span class=\"n\">FORMAT</span> <span class=\"n\">json_format</span> <span class=\"k\">TYPE</span> <span class=\"o\">=</span> <span class=\"n\">JSON</span><span class=\"p\">;</span>\n<span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">REPLACE</span> <span class=\"n\">STAGE</span> <span class=\"n\">mystage</span> <span class=\"n\">FILE_FORMAT</span> <span class=\"o\">=</span> <span class=\"n\">json_format</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>Let’s create a file with some JSON data to load into the table. Run the following in a shell:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">echo</span> <span class=\"s1\">'{\"name\": \"Anne\", \"surname\": \"Houston\", \"age\": 37}'</span> <span class=\"o\">&gt;</span> /tmp/data.json\n<span class=\"nb\">echo</span> <span class=\"s1\">'{\"name\": \"John\", \"surname\": \"Doe\", \"age\": 21}'</span> <span class=\"o\">&gt;&gt;</span> /tmp/data.json\n<span class=\"nb\">echo</span> <span class=\"s1\">'{\"name\": \"William\", \"surname\": \"Williams\", \"age\": 26}'</span> <span class=\"o\">&gt;&gt;</span> /tmp/data.json\n</code></pre></div></div>\n\n<p>Next we run this in snowflake to upload the data to a stage and then load the data into the table:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"n\">put</span> <span class=\"n\">file</span><span class=\"p\">:</span><span class=\"o\">///</span><span class=\"n\">tmp</span><span class=\"o\">/</span><span class=\"k\">data</span><span class=\"p\">.</span><span class=\"n\">json</span> <span class=\"o\">@</span><span class=\"n\">mystage</span>\n<span class=\"k\">COPY</span> <span class=\"k\">INTO</span> <span class=\"n\">clients_raw</span> <span class=\"k\">FROM</span> <span class=\"o\">@</span><span class=\"n\">mystage</span><span class=\"o\">/</span><span class=\"k\">data</span><span class=\"p\">.</span><span class=\"n\">json</span> <span class=\"n\">FILE_FORMAT</span> <span class=\"o\">=</span> <span class=\"n\">json_format</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>We can now query the data as:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">SELECT</span> <span class=\"k\">min</span><span class=\"p\">(</span><span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">age</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">age</span> <span class=\"k\">from</span> <span class=\"n\">clients_raw</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<h3 id=\"creating-a-view\">Creating a view</h3>\n\n<p>It is easy to query the data, but it can be verbose and a little confusing to analysts who have never worked with unstructured data. In order to make it transparent to the end user we can create a view that turns it into structured data.</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">VIEW</span> <span class=\"n\">clients</span> <span class=\"k\">AS</span>\n<span class=\"k\">SELECT</span>\n<span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">name</span><span class=\"p\">::</span><span class=\"nb\">VARCHAR</span> <span class=\"k\">AS</span> <span class=\"n\">name</span><span class=\"p\">,</span>\n<span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">surname</span><span class=\"p\">::</span><span class=\"nb\">VARCHAR</span> <span class=\"k\">AS</span> <span class=\"n\">surname</span><span class=\"p\">,</span>\n<span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">age</span><span class=\"p\">::</span><span class=\"nb\">NUMERIC</span> <span class=\"k\">AS</span> <span class=\"n\">age</span>\n<span class=\"k\">FROM</span> <span class=\"n\">clients_raw</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>The same query from before would now be:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">SELECT</span> <span class=\"k\">min</span><span class=\"p\">(</span><span class=\"n\">age</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">age</span> <span class=\"k\">from</span> <span class=\"n\">clients</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>And now it’s indistinguishable from a structured table from the user’s point of view.</p>\n\n<h3 id=\"removing-a-column-adding-a-column\">Removing a column, adding a column</h3>\n\n<p>Suppose that database admins realized that storing age in a column is not ideal, since it needs to be updated every time a client has a birthday. Instead he decides to drop the age column and store a date with their birthday. The new table is as follows:</p>\n\n<table>\n  <thead>\n    <tr>\n      <th>name</th>\n      <th>surname</th>\n      <th>birthday</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Anne</td>\n      <td>Houston</td>\n      <td>1984-03-12</td>\n    </tr>\n    <tr>\n      <td>John</td>\n      <td>Doe</td>\n      <td>2000-01-03</td>\n    </tr>\n    <tr>\n      <td>William</td>\n      <td>Williams</td>\n      <td>1995-02-04</td>\n    </tr>\n  </tbody>\n</table>\n\n<p>Let’s create the new data:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">echo</span> <span class=\"s1\">'{\"name\": \"Anne\", \"surname\": \"Houston\", \"birthday\": \"1984-03-12\"}'</span> <span class=\"o\">&gt;</span> /tmp/data.json\n<span class=\"nb\">echo</span> <span class=\"s1\">'{\"name\": \"John\", \"surname\": \"Doe\", \"birthday\": \"2000-01-03\"}'</span> <span class=\"o\">&gt;&gt;</span> /tmp/data.json\n<span class=\"nb\">echo</span> <span class=\"s1\">'{\"name\": \"William\", \"surname\": \"Williams\", \"birthday\": \"1995-02-04\"}'</span> <span class=\"o\">&gt;&gt;</span> /tmp/data.json\n</code></pre></div></div>\n\n<p>We would usually append the data, but to make this tutorial simple we will just replace the old data with the new one.</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">TRUNCATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">clients_raw</span><span class=\"p\">;</span>\n<span class=\"k\">COPY</span> <span class=\"k\">INTO</span> <span class=\"n\">clients_raw</span> <span class=\"k\">FROM</span> <span class=\"o\">@</span><span class=\"n\">mystage</span><span class=\"o\">/</span><span class=\"k\">data</span><span class=\"p\">.</span><span class=\"n\">json</span> <span class=\"n\">FILE_FORMAT</span> <span class=\"o\">=</span> <span class=\"n\">json_format</span><span class=\"p\">;</span>\n<span class=\"n\">put</span> <span class=\"n\">file</span><span class=\"p\">:</span><span class=\"o\">///</span><span class=\"n\">tmp</span><span class=\"o\">/</span><span class=\"k\">data</span><span class=\"p\">.</span><span class=\"n\">json</span> <span class=\"o\">@</span><span class=\"n\">mystage</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>Since we store all available data as a variant our integration will not break. The view would not break either, but the age would show as null (try <code class=\"language-plaintext highlighter-rouge\">SELECT * FROM clients</code>). The only thing we need to do to take advantage of the new field is to update the view. For backwards compatibility we will still include the age as a calculation.</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">REPLACE</span> <span class=\"k\">VIEW</span> <span class=\"n\">clients</span> <span class=\"k\">AS</span>\n<span class=\"k\">SELECT</span>\n<span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">name</span><span class=\"p\">::</span><span class=\"nb\">VARCHAR</span> <span class=\"k\">as</span> <span class=\"n\">name</span><span class=\"p\">,</span>\n<span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">surname</span><span class=\"p\">::</span><span class=\"nb\">VARCHAR</span> <span class=\"k\">as</span> <span class=\"n\">surname</span><span class=\"p\">,</span>\n<span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">birthday</span><span class=\"p\">::</span><span class=\"nb\">DATE</span> <span class=\"k\">as</span> <span class=\"n\">birthday</span><span class=\"p\">,</span>\n<span class=\"n\">DATEDIFF</span><span class=\"p\">(</span><span class=\"s1\">'years'</span><span class=\"p\">,</span> <span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">birthday</span><span class=\"p\">,</span> <span class=\"k\">CURRENT_DATE</span><span class=\"p\">())</span> <span class=\"k\">as</span> <span class=\"n\">age</span>\n<span class=\"k\">FROM</span> <span class=\"n\">clients_raw</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>That’s it, our pipelines never broke and there’s no need to change our data flows or source table definitions!</p>\n\n<p>If a new field gets added to the table and no one notices it’s still getting staged into snowflake in the variant so the moment someone requests the field in the view we’ll be able to see it, without needing to backfill the data.</p>\n\n<h3 id=\"doesnt-this-take-up-more-space-than-regular-tables-isnt-it-slower-to-query\">Doesn’t this take up more space than regular tables? Isn’t it slower to query?</h3>\n\n<p>This excerpt from <a href=\"https://docs.snowflake.com/en/user-guide/semistructured-considerations.html#storing-semi-structured-data-in-a-variant-column-vs-flattening-the-nested-structure\">Snowflake’s docs</a> answers the question:</p>\n\n<blockquote>\n  <p>For data that is mostly regular and uses only native types (strings and integers), the storage requirements and query performance for operations on relational data and data in a VARIANT column is very similar.\nFor better pruning and less storage consumption, we recommend flattening your object and key data into separate relational columns if your semi-structured data includes:</p>\n</blockquote>\n\n<ul>\n  <li>Dates and timestamps, especially non-ISO 8601dates and timestamps, as string values</li>\n  <li>Numbers within strings</li>\n  <li>Arrays</li>\n</ul>\n\n<p>Non-native values such as dates and timestamps are stored as strings when loaded into a VARIANT column, so operations on these values could be slower and also consume more space than when stored in a relational column with the corresponding data type.</p>\n<blockquote>\n\n</blockquote>\n\n<p>So in terms of performance and storage it should be really similar albeit a little slower. An exception would be if we need to query the birthday because it’s stored as a string, as we will see in the following section.</p>\n\n<h3 id=\"improving-performance\">Improving performance</h3>\n\n<p>Because variants store dates as strings, they are not as efficient to filter by. This is only an issue if the table is large and you intend to query the table by that date. Let’s see an example of how to improve performance in that case:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">REPLACE</span> <span class=\"k\">TABLE</span> <span class=\"n\">clients_raw</span> <span class=\"p\">(</span><span class=\"n\">src</span> <span class=\"n\">VARIANT</span><span class=\"p\">,</span> <span class=\"n\">birthday</span> <span class=\"nb\">DATE</span><span class=\"p\">);</span>\n<span class=\"k\">COPY</span> <span class=\"k\">INTO</span> <span class=\"n\">clients_raw</span> <span class=\"k\">FROM</span> <span class=\"p\">(</span>\n<span class=\"k\">select</span>\n<span class=\"err\">$</span><span class=\"mi\">1</span> <span class=\"k\">as</span> <span class=\"n\">src</span><span class=\"p\">,</span>\n<span class=\"n\">to_date</span><span class=\"p\">(</span><span class=\"err\">$</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">birthday</span><span class=\"p\">)::</span><span class=\"nb\">DATE</span> <span class=\"k\">AS</span> <span class=\"n\">birthday</span>\n<span class=\"k\">FROM</span> <span class=\"o\">@</span><span class=\"n\">mystage</span><span class=\"o\">/</span><span class=\"k\">data</span><span class=\"p\">.</span><span class=\"n\">json</span>\n<span class=\"p\">)</span> <span class=\"n\">FILE_FORMAT</span> <span class=\"o\">=</span> <span class=\"n\">json_format</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>And modify the view:</p>\n\n<div class=\"language-sql highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"k\">CREATE</span> <span class=\"k\">OR</span> <span class=\"k\">REPLACE</span> <span class=\"k\">VIEW</span> <span class=\"n\">clients</span>\n<span class=\"k\">SELECT</span>\n<span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">name</span><span class=\"p\">::</span><span class=\"nb\">VARCHAR</span> <span class=\"k\">as</span> <span class=\"n\">name</span><span class=\"p\">,</span>\n<span class=\"n\">src</span><span class=\"p\">:</span><span class=\"n\">surname</span><span class=\"p\">::</span><span class=\"nb\">VARCHAR</span> <span class=\"k\">as</span> <span class=\"n\">surname</span><span class=\"p\">,</span>\n<span class=\"n\">birthday</span><span class=\"p\">,</span>     <span class=\"o\">//</span> <span class=\"o\">&lt;</span><span class=\"c1\">-- We changed this to get the column directly</span>\n<span class=\"n\">DATEDIFF</span><span class=\"p\">(</span><span class=\"s1\">'years'</span><span class=\"p\">,</span> <span class=\"n\">birthday</span><span class=\"p\">,</span> <span class=\"k\">CURRENT_DATE</span><span class=\"p\">())</span> <span class=\"k\">as</span> <span class=\"n\">age</span>  <span class=\"o\">//</span> <span class=\"o\">&lt;</span><span class=\"c1\">-- Here too</span>\n<span class=\"k\">FROM</span> <span class=\"n\">clients_raw</span><span class=\"p\">;</span>\n</code></pre></div></div>\n\n<p>Now queries filtering by birthday (or getting <code class=\"language-plaintext highlighter-rouge\">MAX(birthday)</code> for example) will be much faster.</p>\n\n<h2 id=\"what-is-the-best-way-to-load-the-data\">What is the best way to load the data?</h2>\n\n<p>The most efficient way to load the data into a table is by using a COPY command since Snowflake can optimize a bulk load. It can’t do that with insert statements. Here are some of the most popular ways to load the data into snowflake, each with their advantges and disadvantages:</p>\n\n<ul>\n  <li>CSV: A gzipped CSV is the fastest way to load structured data into snowflake. It takes more space than parquet. It can also be loaded into a variant column with the right casting (see example later). The data can not be loaded easily into a variant.</li>\n  <li>JSON: Can be easily loaded into a variant, but it takes a lot of space in your data lake.</li>\n  <li>Avro: Built-in schema, easily loaded into a variant or into a structured table. Takes more space than parquet.</li>\n  <li>Parquet: Columnar storage that has a better compression than the other options and can easily be loaded into a structured or unstructured table. It is slower than CSV to load into a structured table.</li>\n</ul>\n\n<h2 id=\"bottom-line\">Bottom Line</h2>\n\n<p>Loading data into Snowflake using this method is a great way to save you a lot of headaches and minimise data pipeline failures. It is a good rule of thumb to always use this method unless you will be loading an extremely large amount of data and need the extra 20% performance that a  structured table will give you. If you decide to create a structured table instead of using this method be aware that the pipelines will break on any schema change.</p>\n\n<h2 id=\"what-are-the-best-tools-to-load-data-like-this\">What are the best tools to load data like this?</h2>\n\n<p>Any ETL/ELT tool that is flexible enough, for instance Airflow can be adapted to use this method. You can also check out our ETL tool, <a href=\"/\">Typhoon</a>, that encourages this pattern and other modern best practices for data engineering.</p>\n\n<h2 id=\"sources\">Sources</h2>\n\n<p><a href=\"https://www.snowflake.com/wp-content/uploads/2015/06/Snowflake_Semistructured_Data_WP_1_0_062015.pdf\">https://www.snowflake.com/wp-content/uploads/2015/06/Snowflake_Semistructured_Data_WP_1_0_062015.pdf</a></p>\n\n<p><a href=\"https://docs.snowflake.com/en/user-guide/semistructured-considerations.html#storing-semi-structured-data-in-a-variant-column-vs-flattening-the-nested-structure\">https://docs.snowflake.com/en/user-guide/semistructured-considerations.html#storing-semi-structured-data-in-a-variant-column-vs-flattening-the-nested-structure</a></p>","excerpt":"Table of contents\n\n  Table of contents","languages":null,"categories":["article"],"tags":["typhoon","Snowflake","Modern data stack","article","ETL"]},{"title":"How to create a Serverless Telegram Bot to send daily jokes to your friends","permalink":"https://typhoondata.io/serverless-telegram-bot-jokes.html","link":"https://typhoondata.io/serverless-telegram-bot-jokes.html","date":"2022-05-31T00:00:00-07:00","modified":"2022-07-25T22:22:06-07:00","author":{"name":"Typhoon Data","url":"https://typhoondata.io","email":"info.typhoon.data@gmail.com"},"content":"<h4 id=\"table-of-contents\">Table of contents</h4>\n<ul id=\"markdown-toc\">\n  <li><a href=\"#table-of-contents\" id=\"markdown-toc-table-of-contents\">Table of contents</a></li>\n  <li><a href=\"#summary\" id=\"markdown-toc-summary\">Summary</a></li>\n  <li><a href=\"#getting-started\" id=\"markdown-toc-getting-started\">Getting started</a></li>\n  <li><a href=\"#tell-me-a-joke\" id=\"markdown-toc-tell-me-a-joke\">Tell me a joke!</a></li>\n  <li><a href=\"#i-want-the-joke-on-telegram\" id=\"markdown-toc-i-want-the-joke-on-telegram\">I want the joke on telegram</a></li>\n  <li><a href=\"#aiming-for-the-clouds\" id=\"markdown-toc-aiming-for-the-clouds\">Aiming for the clouds</a>    <ul>\n      <li><a href=\"#build-and-upload-the-workflow\" id=\"markdown-toc-build-and-upload-the-workflow\">Build and upload the workflow</a></li>\n      <li><a href=\"#deploying-infrastructure\" id=\"markdown-toc-deploying-infrastructure\">Deploying infrastructure</a></li>\n    </ul>\n  </li>\n  <li><a href=\"#lets-take-it-for-a-spin\" id=\"markdown-toc-lets-take-it-for-a-spin\">Let’s take it for a spin</a></li>\n  <li><a href=\"#why-cant-i-just-run-everything-in-one-lambda\" id=\"markdown-toc-why-cant-i-just-run-everything-in-one-lambda\">Why can’t I just run everything in one lambda?</a></li>\n  <li><a href=\"#this-is-good-to-be-true-can-i-really-build-all-my-etls-like-this\" id=\"markdown-toc-this-is-good-to-be-true-can-i-really-build-all-my-etls-like-this\">This is good to be true, can I really build all my ETLs like this?</a></li>\n  <li><a href=\"#does-that-mean-that-typhoon-is-not-ready-for-prime-time\" id=\"markdown-toc-does-that-mean-that-typhoon-is-not-ready-for-prime-time\">Does that mean that Typhoon is not ready for prime time?</a></li>\n  <li><a href=\"#cleaning-up\" id=\"markdown-toc-cleaning-up\">Cleaning up</a></li>\n  <li><a href=\"#thanks-for-following-along\" id=\"markdown-toc-thanks-for-following-along\">Thanks for following along!</a></li>\n</ul>\n\n<h4 id=\"summary\">Summary</h4>\n\n<p><a href=\"https://github.com/typhoon-data-org/typhoon-orchestrator\">Typhoon Orchestrator</a> is a great way to deploy ETL workflow on AWS Lambda. In this tutorial we intend to show how easy to use and versatile it is by deploying code to Lambda that gets a random joke from <a href=\"https://jokeapi.dev\">https://jokeapi.dev</a> once a day and sends it to your telegram group.</p>\n\n<h2 id=\"getting-started\">Getting started</h2>\n\n<p>The first thing you need to do is install typhoon and the rest of the dependencies needed for this tutorial, preferrably in a virtual environment.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>pip <span class=\"nb\">install </span>typhoon-orchestrator[dev]\npip <span class=\"nb\">install </span>python-telegram-bot\npip <span class=\"nb\">install </span>requests\n</code></pre></div></div>\n\n<p>Next we create our project, we will call our project jester (we could call it anything).</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>typhoon init jester <span class=\"nt\">--template</span> minimal\n<span class=\"nb\">cd </span>jester\ntyphoon status\n</code></pre></div></div>\n\n<p>Notice that the status command gives us the following warning: <code class=\"language-plaintext highlighter-rouge\">Connections YAML not found. To add connections create connections.yml</code>. This is normal because typhoon normally uses a metadata database where you can store connections and variables, but we don’t want to create and use any DynamoDB tables for this tutorial so we used the minimal template that doesn’t include anything related to the metadata database. If you see any warnings about the metadata database during the course of the tutorial don’t worry, it’s for the same reason.</p>\n\n<h2 id=\"tell-me-a-joke\">Tell me a joke!</h2>\n\n<p>Before we worry about telegram, let’s create a workflow that calls the joke API and prints the joke on your CLI. Create the file: <code class=\"language-plaintext highlighter-rouge\">dags/send_me_a_joke.yml</code>:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">send_me_a_joke</span>\n<span class=\"na\">schedule_interval</span><span class=\"pi\">:</span> <span class=\"s1\">'</span><span class=\"s\">@daily'</span>\n\n<span class=\"na\">tasks</span><span class=\"pi\">:</span>\n  <span class=\"na\">get_joke</span><span class=\"pi\">:</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.http.get_raw</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">url</span><span class=\"pi\">:</span> <span class=\"s\">https://v2.jokeapi.dev/joke/Programming?blacklistFlags=nsfw,religious,political,racist,sexist,explicit&amp;type=single</span>\n  \n  <span class=\"na\">select_joke_text</span><span class=\"pi\">:</span>\n    <span class=\"na\">input</span><span class=\"pi\">:</span> <span class=\"s\">get_joke</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.json.search</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">data</span><span class=\"pi\">:</span> <span class=\"kt\">!Py</span> <span class=\"s\">$BATCH.response.json()</span>\n      <span class=\"na\">expression</span><span class=\"pi\">:</span> <span class=\"s\">joke</span>\n\n  <span class=\"na\">tell_joke</span><span class=\"pi\">:</span>\n    <span class=\"na\">input</span><span class=\"pi\">:</span> <span class=\"s\">select_joke_text</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.debug.echo</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">joke</span><span class=\"pi\">:</span> <span class=\"kt\">!Py</span> <span class=\"s\">$BATCH</span>\n</code></pre></div></div>\n\n<p>This workflow has three tasks using built-in functions:</p>\n\n<ul>\n  <li>\n    <p><strong>get_joke</strong>: Calls the joke API and gets a response like to the following:</p>\n\n    <div class=\"language-json highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"w\">  </span><span class=\"p\">{</span><span class=\"w\">\n      </span><span class=\"nl\">\"error\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"category\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"Programming\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"type\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"single\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"joke\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"A man is smoking a cigarette and blowing smoke rings into the air. His girlfriend becomes irritated with the smoke and says </span><span class=\"se\">\\\"</span><span class=\"s2\">Can't you see the warning on the cigarette pack? Smoking is hazardous to your health!</span><span class=\"se\">\\\"</span><span class=\"s2\"> to which the man replies, </span><span class=\"se\">\\\"</span><span class=\"s2\">I am a programmer.  We don't worry about warnings; we only worry about errors.</span><span class=\"se\">\\\"</span><span class=\"s2\">\"</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"flags\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"p\">{</span><span class=\"w\">\n          </span><span class=\"nl\">\"nsfw\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"religious\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"political\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"racist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"sexist\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"p\">,</span><span class=\"w\">\n          </span><span class=\"nl\">\"explicit\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">false</span><span class=\"w\">\n      </span><span class=\"p\">},</span><span class=\"w\">\n      </span><span class=\"nl\">\"id\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"mi\">38</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"safe\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"p\">,</span><span class=\"w\">\n      </span><span class=\"nl\">\"lang\"</span><span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">\"en\"</span><span class=\"w\">\n  </span><span class=\"p\">}</span><span class=\"w\">\n</span></code></pre></div>    </div>\n  </li>\n  <li><strong>select_joke_text</strong>: Uses a <a href=\"https://jmespath.org/\">JMESPath</a> expression to select data from the JSON text.</li>\n  <li><strong>tell_joke</strong>: Prints the joke text.</li>\n</ul>\n\n<p>The <code class=\"language-plaintext highlighter-rouge\">!Py</code> tag means that instead of passing it a YAML object, you are passing it a string representing python code to run. For example, <code class=\"language-plaintext highlighter-rouge\">foo: 4</code> is equivalent to <code class=\"language-plaintext highlighter-rouge\">foo: !Py 2+2</code>.  <code class=\"language-plaintext highlighter-rouge\">$BATCH</code> is a special variable that holds whatever the previous function returned or yielded. In the case of the <code class=\"language-plaintext highlighter-rouge\">select_joke_test</code> task where the input is the <code class=\"language-plaintext highlighter-rouge\">get_joke</code> task, its function returned a NamedTuple with a response and some metadata, so that <code class=\"language-plaintext highlighter-rouge\">$BATCH.response</code>is a requests.Response object.</p>\n\n<p>Lets run to see a joke in our terminal</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>typhoon dag run <span class=\"nt\">--dag-name</span> send_me_a_joke\n</code></pre></div></div>\n\n<p>Piece of cake! But here comes the interesting part…</p>\n\n<h2 id=\"i-want-the-joke-on-telegram\">I want the joke on telegram</h2>\n\n<p>There is no built-in function in Typhoon to send a text to a telegram chat. Fortunately it’s very easy to extend Typhoon, so let’s make it ourselves.</p>\n\n<p>Create the following file <code class=\"language-plaintext highlighter-rouge\">functions/msg.py</code>:</p>\n\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"kn\">import</span> <span class=\"nn\">telegram</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">send_message_telegram</span><span class=\"p\">(</span><span class=\"n\">token</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">chat_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"s\">\"\"\"Given a telegram bot token, chat_id and message,\n       send the message to that chat\"\"\"</span>\n    <span class=\"n\">bot</span> <span class=\"o\">=</span> <span class=\"n\">telegram</span><span class=\"p\">.</span><span class=\"n\">Bot</span><span class=\"p\">(</span><span class=\"n\">token</span><span class=\"o\">=</span><span class=\"n\">token</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s\">'Sending message </span><span class=\"si\">{</span><span class=\"n\">message</span><span class=\"si\">}</span><span class=\"s\"> to </span><span class=\"si\">{</span><span class=\"n\">chat_id</span><span class=\"si\">}</span><span class=\"s\">'</span><span class=\"p\">)</span>\n    <span class=\"n\">bot</span><span class=\"p\">.</span><span class=\"n\">send_message</span><span class=\"p\">(</span><span class=\"n\">chat_id</span><span class=\"o\">=</span><span class=\"n\">chat_id</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">message</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">message</span>\n</code></pre></div></div>\n\n<p>And update the DAG file we created before at <code class=\"language-plaintext highlighter-rouge\">dags/send_me_a_joke.yml</code>:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">name</span><span class=\"pi\">:</span> <span class=\"s\">send_me_a_joke</span>\n<span class=\"na\">schedule_interval</span><span class=\"pi\">:</span> <span class=\"s\">0 10 * * *</span>  <span class=\"c1\"># Send the joke at 10am every day</span>\n\n<span class=\"na\">tasks</span><span class=\"pi\">:</span>\n  <span class=\"na\">get_joke</span><span class=\"pi\">:</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.http.get_raw</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">url</span><span class=\"pi\">:</span> <span class=\"s\">https://v2.jokeapi.dev/joke/Programming?blacklistFlags=nsfw,religious,political,racist,sexist,explicit&amp;type=single</span>\n  \n  <span class=\"na\">select_joke_text</span><span class=\"pi\">:</span>\n    <span class=\"na\">input</span><span class=\"pi\">:</span> <span class=\"s\">get_joke</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.json.search</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">data</span><span class=\"pi\">:</span> <span class=\"kt\">!Py</span> <span class=\"s\">$BATCH.response.json()</span>\n      <span class=\"na\">expression</span><span class=\"pi\">:</span> <span class=\"s\">joke</span>\n\n  <span class=\"na\">tell_joke</span><span class=\"pi\">:</span>\n    <span class=\"na\">input</span><span class=\"pi\">:</span> <span class=\"s\">select_joke_text</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">functions.msg.send_message_telegram</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">message</span><span class=\"pi\">:</span> <span class=\"kt\">!Py</span> <span class=\"s\">$BATCH</span>\n      <span class=\"na\">token</span><span class=\"pi\">:</span> <span class=\"kt\">!Var</span> <span class=\"s\">telegram_token</span>\n      <span class=\"na\">chat_id</span><span class=\"pi\">:</span> <span class=\"kt\">!Var</span> <span class=\"s\">chat_id</span>\n\n<span class=\"na\">requirements</span><span class=\"pi\">:</span>\n  <span class=\"pi\">-</span> <span class=\"s\">python-telegram-bot</span>\n  <span class=\"pi\">-</span> <span class=\"s\">requests</span>\n</code></pre></div></div>\n\n<p>Notice that for the token and chat id we have the <code class=\"language-plaintext highlighter-rouge\">!Var</code> tag. This is because we don’t want to include a secret like a token in the code, so we will read it from a variable. If you are really perceptive you may be thinking: “Didn’t you say that we are using a minimal deployment where there is no metadata database to store variables on?” Yes, that’s 100% correct. Usually we would store variables in the metadata database. However, we will use the alternate method of storing variables which is using an environment variable that starts with <code class=\"language-plaintext highlighter-rouge\">TYPHOON_VARIABLE_</code>.</p>\n\n<ul>\n  <li>To create a bot with the botfather and <strong>get a token</strong> follow the official tutorial <a href=\"https://core.telegram.org/bots#creating-a-new-bot\">https://core.telegram.org/bots#creating-a-new-bot</a></li>\n  <li>To <strong>find out your chat ID</strong> check out <a href=\"https://stackoverflow.com/questions/32423837/telegram-bot-how-to-get-a-group-chat-id\">https://stackoverflow.com/questions/32423837/telegram-bot-how-to-get-a-group-chat-id</a>. Keep in mind that <strong>you can only add the bot to group chats, not private conversations</strong>.</li>\n</ul>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">TYPHOON_VARIABLE_telegram_token</span><span class=\"o\">=</span><span class=\"s2\">\"MY_SECRET_TELEGRAM_TOKEN\"</span>\n<span class=\"nb\">export </span><span class=\"nv\">TYPHOON_VARIABLE_chat_id</span><span class=\"o\">=</span><span class=\"s2\">\"128332492187641\"</span>\n</code></pre></div></div>\n\n<p>Now that we have everything ready, let’s send some jokes.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>typhoon dag run <span class=\"nt\">--dag-name</span> send_me_a_joke\n</code></pre></div></div>\n\n<p>If everything was correctly set up you should get the notification with a random programmer joke!</p>\n\n<h2 id=\"aiming-for-the-clouds\">Aiming for the clouds</h2>\n\n<h3 id=\"build-and-upload-the-workflow\">Build and upload the workflow</h3>\n\n<p>This is all well and good, but we want the bot to tell us a joke every day without needing to run the code locally. First of all let’s compile our code into a zip and upload it to S3 so that Lambda can use it. This can be a little tedious, but luckily Typhoon takes care of that for us. We need to tell it to which S3 bucket we want to deploy to. <strong>You will also need a configured <a href=\"https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html#cli-configure-profiles-create\">AWS profile</a></strong>. Open the <code class=\"language-plaintext highlighter-rouge\">.typhoonremotes</code> file and modify it to use your profile and S3 bucket.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>[test]\naws-profile=myaws\ns3-bucket=typhoon-orchestrator\n</code></pre></div></div>\n\n<p>Now that we have a remote called <code class=\"language-plaintext highlighter-rouge\">test</code> we are ready to create the zip files and push them to S3. You will need to have docker installed for this step because the dependencies need to be built in an OS that is compatible with the one Lambda is using, otherwise they won’t work. This is a very common source of problems that Typhoon helps you avoid. If you are sure that your OS is compatible you can add the flag <code class=\"language-plaintext highlighter-rouge\">--build-deps-locally</code>, but <strong>it is generally not recomended</strong>.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>typhoon dag push --dag-name send_me_a_joke test\n</code></pre></div></div>\n\n<p>This will have taken a very long time because Typhoon built all of the dependencies, but don’t worry <strong>updating the workflow code is much much faster since the dependencies are separated into a layer and don’t need to be re-deployed unless they change</strong>.</p>\n\n<p>The <code class=\"language-plaintext highlighter-rouge\">test</code> at the end tells it what remote to deploy to. In the future we could add a different production environment with its own remote.</p>\n\n<p>If you check your S3 bucket now you’ll find two files:</p>\n\n<ul>\n  <li><strong>The lambda code:</strong> <code class=\"language-plaintext highlighter-rouge\">typhoon_dag_builds/send_me_a_joke/lambda.zip</code></li>\n  <li><strong>All the necessary dependencies</strong>: <code class=\"language-plaintext highlighter-rouge\">typhoon_dag_builds/send_me_a_joke/layer.zip</code></li>\n</ul>\n\n<p><img src=\"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/gfjtgfcfs5q19ay8jvrp.png\" alt=\"S3 objects\" /></p>\n\n<h3 id=\"deploying-infrastructure\">Deploying infrastructure</h3>\n\n<p>For this part you will need to <a href=\"https://learn.hashicorp.com/tutorials/terraform/install-cli\">install and set up terraform</a>. Learn more about infrastructure as code <a href=\"https://learn.hashicorp.com/tutorials/terraform/infrastructure-as-code?in=terraform/aws-get-started\">here</a>.</p>\n\n<p><strong>Typhoon automatically creates some terraform files that describe all the necessary infrastructure</strong> to create in order to deploy our workflow to AWS Lambda. This greatly simplifies the creation of all the necessary resources that you would otherwise need to create manually. More importantly, it provides you a starting point while also giving you full control to change the terraform files until you have the desired configuration.</p>\n\n<p>For this tutorial you just need to <strong>update the test variables file to include the S3 bucket name and some DAG info</strong>. We can get the info for all the dags by running <code class=\"language-plaintext highlighter-rouge\">typhoon dag info --json-output --indent 2</code>, but in this case we will need to adapt it to include the necessary environment variables. This means that you will need to add the following to the file <code class=\"language-plaintext highlighter-rouge\">terraform/test.tfvars</code>.</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>dag_info = {\n    \"send_me_a_joke\": {\n        \"schedule_interval\": \"cron(0 10 * * ? *)\",\n        \"environment\": {\n            \"TYPHOON_VARIABLE_telegram_token\": \"MY_SECRET_TELEGRAM_TOKEN\",\n            \"TYPHOON_VARIABLE_chat_id\": \"128332492187641\"\n        }\n    }\n}\n</code></pre></div></div>\n\n<p>Notice how the schedule interval is in a different format than the one we defined. This is because Terraform maps to AWS resources, and AWS uses its own flavor of cron expressions which is incompatible with the standard Unix cron expressions used by tools like cron, crontab, Airflow and many more. Typhoon aims to be a framework that can deploy to many platforms (currently supports AWS Lambda and Airflow) so we decided to follow the industry standard instead of AWS’s. Luckily, when we run <code class=\"language-plaintext highlighter-rouge\">typhoon dag info ...</code> <strong>Typhoon converts it to AWS’s standard so you don’t need to do that yourself!</strong></p>\n\n<p>Now we are ready to create the infrastructure with terraform.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"nb\">export </span><span class=\"nv\">AWS_PROFILE</span><span class=\"o\">=</span>my-aws-profile\n<span class=\"nb\">export </span><span class=\"nv\">AWS_DEFAULT_REGION</span><span class=\"o\">=</span>eu-west-1\n<span class=\"nb\">cd </span>terraform\nterraform init\nterraform plan <span class=\"nt\">-var-file</span><span class=\"o\">=</span>test.tfvars <span class=\"nt\">-out</span><span class=\"o\">=</span>tfplan\nterraform apply tfplan\n</code></pre></div></div>\n\n<p>And voila! You can check all of the resources that have been created in AWS and take a moment to appreciate how much time we’ve saved.</p>\n\n<p><img src=\"https://dev-to-uploads.s3.amazonaws.com/uploads/articles/ymarmji5bglvr7axo16r.png\" alt=\"AWS Lambda Function\" /></p>\n\n<h2 id=\"lets-take-it-for-a-spin\">Let’s take it for a spin</h2>\n\n<p>If everything worked correctly you will get a joke in your telegram chat at 10am, but we don’t want to wait that long, we want to hear one now! You could invoke the Lambda from the AWS console, but we will invoke it with Typhoon.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>typhoon dag run <span class=\"nt\">--dag-name</span> send_me_a_joke <span class=\"nb\">test</span>\n</code></pre></div></div>\n\n<p>Hopefully you got a hilarious joke sent right to your group chat.</p>\n\n<p>This is the same command we used earlier to run the workflow locally, but with <code class=\"language-plaintext highlighter-rouge\">test</code>at the end specifying that we want to run it in the remote environment. This has invoked a lambda and shown you the logs. Actually, to be more precise, it has invoked a Lambda that has then invoked another Lambda and then invoked another Lambda. Why? Because Typhoon is asynchronous by default which means that as soon as a function returns or yields a batch we invoke a new Lambda to process it. This is useful because you can have a lot of tasks performing work in parallel. For example, imagine you have a workflow that reads FTP CSV files, zips them up and uploads to S3. The first task could list all the CSV files in the FTP and yields each path as a batch. Then the next task will compress them which can take a long time, but we actually invoked a new Lambda instance for each batch so we are processing them all in parallel!</p>\n\n<p>Notice how even though the workflow ran across three lambdas, you still got the full log in your terminal. Lambdas can be hard to monitor and debug, but Typhoon tries to make this process easier. This is why when you run a Typhoon DAG manually, it waits for a response so that it can print the logs. Every invocation will in turn also wait for the response of any Lambdas it invokes so you will end up with the full log no matter how many Lambda invocations the workflow ran on. It’s extremely useful to be able to see if the DAG is working correctly, but it does introduce synchronicity so the DAG will run slower. We believe it’s a worthwile tradeoff for manual invocations. <strong>Rest assured that when the workflow is triggered on schedule it will run at full speed</strong>.</p>\n\n<h2 id=\"why-cant-i-just-run-everything-in-one-lambda\">Why can’t I just run everything in one lambda?</h2>\n\n<p>Great question, and there’s no reason not to since our worflow is very light and doesn’t benefit from parallelism. You just need to modify the first two tasks to make them synchronous with <code class=\"language-plaintext highlighter-rouge\">asynchronous: False</code>. This is the relevant part of the code:</p>\n\n<div class=\"language-yaml highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"na\">tasks</span><span class=\"pi\">:</span>\n  <span class=\"na\">get_joke</span><span class=\"pi\">:</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.http.get_raw</span>\n    <span class=\"na\">asynchronous</span><span class=\"pi\">:</span> <span class=\"no\">false</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">url</span><span class=\"pi\">:</span> <span class=\"s\">https://v2.jokeapi.dev/joke/Programming?blacklistFlags=nsfw,religious,political,racist,sexist,explicit&amp;type=single</span>\n  \n  <span class=\"na\">select_joke_text</span><span class=\"pi\">:</span>\n    <span class=\"na\">input</span><span class=\"pi\">:</span> <span class=\"s\">get_joke</span>\n    <span class=\"na\">function</span><span class=\"pi\">:</span> <span class=\"s\">typhoon.json.search</span>\n    <span class=\"na\">asynchronous</span><span class=\"pi\">:</span> <span class=\"no\">false</span>\n    <span class=\"na\">args</span><span class=\"pi\">:</span>\n      <span class=\"na\">data</span><span class=\"pi\">:</span> <span class=\"kt\">!Py</span> <span class=\"s\">$BATCH.response.json()</span>\n      <span class=\"na\">expression</span><span class=\"pi\">:</span> <span class=\"s\">joke</span>\n</code></pre></div></div>\n\n<p>Lets build and deploy the code, this time without dependencies by using the flag <code class=\"language-plaintext highlighter-rouge\">--code</code>.</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>typhoon dag push <span class=\"nt\">--dag-name</span> send_me_a_joke <span class=\"nb\">test</span> <span class=\"nt\">--code</span>\n</code></pre></div></div>\n\n<p>Wow, that was much faster! You can see that once the workflow has been deployed one time with all the dependencies, <strong>making changes and deploying them is very fast and easy</strong>. Feel free to run the DAG again to check out how only one Lambda will be invoked now.</p>\n\n<h2 id=\"this-is-good-to-be-true-can-i-really-build-all-my-etls-like-this\">This is good to be true, can I really build all my ETLs like this?</h2>\n\n<p>Yes and no… Depending on your use case Lambda can be a good fit, but there are currently some limitations to this approach:</p>\n\n<ul>\n  <li><strong>Lambdas can only run for 15 minutes</strong>. If you have a long running task this will not work for you. In the future <strong>we intend to support Fargate to run heavier tasks</strong> and solve this issue.</li>\n  <li><strong>Can we really do away with the scheduler?</strong> We have shown you a utopian vision of the future of ETLs. It still remains to be seen if we can fully avoid running a scheduler, and we may run into the harsh reality that if you want to be able to implement sensors, rate-limit tasks, etc. we may need a scheduler. Even if that turns out to be true, it would always be opt-in and much simpler than a traditional one.</li>\n</ul>\n\n<h2 id=\"does-that-mean-that-typhoon-is-not-ready-for-prime-time\">Does that mean that Typhoon is not ready for prime time?</h2>\n\n<p><strong>Absolutely not!</strong> We may have a long (albeit exciting) path ahead to realize our vision of a battle tested, fully serverless, asynchronous workflow orchestrator, but AWS is not the only target. <strong>Typhoon supports compilation to native Airflow code</strong>, the most popular orchestrator around today. This feature can bridge the gap between the simplicity of our vision and the complex reality we currently live in as Data Engineers.</p>\n\n<p>Our hope is that you will use Typhoon and fall in love with the simplicity of our vision, and deploy to Airflow if the current state of AWS deployment can’t meet your needs.</p>\n\n<h2 id=\"cleaning-up\">Cleaning up</h2>\n\n<p>If you want to clean up all the resources that were created on this tutorial run the following command:</p>\n\n<div class=\"language-bash highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>terraform plan <span class=\"nt\">-var-file</span><span class=\"o\">=</span>test.tfvars <span class=\"nt\">-out</span><span class=\"o\">=</span>tfplan <span class=\"nt\">-destroy</span>\nterraform apply <span class=\"nt\">-destroy</span> tfplan\n</code></pre></div></div>\n\n<h2 id=\"thanks-for-following-along\">Thanks for following along!</h2>\n\n<p>If you enjoyed this tutorial we hope to see you soon at <a href=\"https://github.com/typhoon-data-org/typhoon-orchestrator\">https://github.com/typhoon-data-org/typhoon-orchestrator</a>. Check out the code, leave a star, open an issue or come say hi on our discord!</p>","excerpt":"Table of contents\n\n  Table of contents","languages":null,"categories":["article"],"tags":["typhoon","serverless","telegram","tutorial","article"]}]